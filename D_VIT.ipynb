{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, RandomRotation, RandomHorizontalFlip, ColorJitter, RandomGrayscale, GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFLWDataset(Dataset):\n",
    "    def __init__(self, annotation_file, base_image_dir, img_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Dataset for WFLW with resizing and augmentations.\n",
    "        Args:\n",
    "            annotation_file: Path to the file containing keypoints annotations.\n",
    "            base_image_dir: Directory containing images.\n",
    "            img_size: Target size for resizing images.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(annotation_file, sep='\\s+', header=None)\n",
    "        self.base_image_dir = base_image_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Define augmentations\n",
    "        self.transforms = Compose([\n",
    "            Resize(img_size),  # Resize image\n",
    "            RandomRotation(10),  # Random rotation\n",
    "            RandomHorizontalFlip(),  # Random horizontal flip\n",
    "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "            RandomGrayscale(p=0.1),  # Convert to grayscale with 10% probability\n",
    "            GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),  # Random blur\n",
    "            ToTensor(),  # Convert to tensor\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "\n",
    "        # Construct image name as \"wflw_train_with_box_{idx}.jpg\"\n",
    "        img_name = f\"wflw_train_with_box_{idx + 1}.jpg\"  # Assuming idx starts from 0, and image names start from 1\n",
    "        img_path = os.path.join(self.base_image_dir, img_name)\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms (resize and augmentations)\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        # Extract keypoints (normalized x and y coordinates)\n",
    "        keypoints_data = row.iloc[1:].values.astype('float').reshape(-1, 2)\n",
    "\n",
    "        # Scale keypoints according to the image size\n",
    "        keypoints_data[:, 0] = keypoints_data[:, 0] * self.img_size[0]  # Scale x\n",
    "        keypoints_data[:, 1] = keypoints_data[:, 1] * self.img_size[1]  # Scale y\n",
    "\n",
    "        return image, torch.tensor(keypoints_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = WFLWDataset(\n",
    "    annotation_file='WFLW/train.txt',\n",
    "    base_image_dir='WFLW/train'\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.reshape(B, C // 4, 2, 2, H, W).permute(0, 1, 4, 2, 5, 3)\n",
    "        return x.reshape(B, C // 4, H * 2, W * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
    "        super(ViT, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSplitViT(nn.Module):\n",
    "    def __init__(self, input_channels=256, embed_dim=512, num_heads=8):\n",
    "        super(ChannelSplitViT, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=2, stride=2)  # Downsampling to (512, 16, 16)\n",
    "        self.vit = ViT(256, num_heads, embed_dim * 4)  # Standard ViT with input (512, 256)\n",
    "        self.pixel_shuffle = PixelShuffle()  # Upsampling to (128, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # (256, 32, 32) → (512, 16, 16)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, H * W).permute(0, 1, 2)  # (512, 16, 16) → (512, 256)\n",
    "        x = self.vit(x)  # (512, 256) → (512, 256)\n",
    "        x = x.permute(0, 1, 2).view(B, C, H, W)  # (512, 256) → (512, 16, 16)\n",
    "        x = self.pixel_shuffle(x)  # (512, 16, 16) → (128, 32, 32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialSplitViT(nn.Module):\n",
    "    def __init__(self, input_channels=256, embed_dim=512, num_heads=8):\n",
    "        super(SpatialSplitViT, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=2, stride=2)  # Downsampling to (512, 16, 16)\n",
    "        self.vit = ViT(embed_dim, num_heads, embed_dim * 4)  # Standard ViT with input (256, 512)\n",
    "        self.pixel_shuffle = PixelShuffle()  # Upsampling to (128, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # (256, 32, 32) → (512, 16, 16)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, H * W).permute(0, 2, 1)  # (512, 16, 16) → (256, 512)\n",
    "        x = self.vit(x)  # (256, 512) → (256, 512)\n",
    "        x = x.permute(0, 2, 1).view(B, C, H, W)  # (256, 512) → (512, 16, 16)\n",
    "        x = self.pixel_shuffle(x)  # (512, 16, 16) → (128, 32, 32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualViT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(DualViT, self).__init__()\n",
    "        self.spatial_vit = SpatialSplitViT()  # Handles spatial splitting\n",
    "        self.channel_vit = ChannelSplitViT()  # Handles channel splitting\n",
    "        # self.merge_conv = nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=1)  # To merge the outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First DualViT\n",
    "        spatial_out = self.spatial_vit(x)  # Output: (B, embed_dim, 32, 32)\n",
    "        # Second DualViT\n",
    "        channel_out = self.channel_vit(x)  # Output: (B, embed_dim, 32, 32)\n",
    "        output = torch.cat([spatial_out, channel_out], dim=1)  # Concatenate along channel dimension\n",
    "        # Further layers\n",
    "        # output = self.merge_conv(output)  # Merge operation\n",
    "        # print(f\"After merge_conv: {output.shape}\")\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_landmarks):\n",
    "        super(PredictionBlock, self).__init__()\n",
    "        self.d_vit1 = DualViT(embed_dim, num_heads)  # First DualViT\n",
    "        self.d_vit2 = DualViT(embed_dim, num_heads)  # Second DualViT\n",
    "        self.final_conv = nn.Conv2d(embed_dim, num_landmarks, kernel_size=1)  # Heatmap prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: First DualViT\n",
    "        x = self.d_vit1(x)  # Output: (B, embed_dim, 32, 32)\n",
    "\n",
    "        # Step 2: Second DualViT\n",
    "        x = self.d_vit2(x)  # Output: (B, embed_dim, 32, 32)\n",
    "\n",
    "        # Step 3: Predict heatmaps\n",
    "        heatmaps = self.final_conv(x)  # Output: (B, num_landmarks, 32, 32)\n",
    "\n",
    "        return x, heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet50, self).__init__()\n",
    "        # Initialize the ResNet backbone up to layer4\n",
    "        self.resnet = ResNet(block=Bottleneck, layers=[3, 4, 6, 3])\n",
    "\n",
    "        # Remove the fully connected layer and adaptive pooling\n",
    "        self.resnet.avgpool = nn.Identity()  # Disable AdaptiveAvgPool2d\n",
    "        self.resnet.fc = nn.Identity()  # Disable the final fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        \n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_argmax(heatmaps):\n",
    "    \"\"\"\n",
    "    Compute soft-argmax for heatmaps.\n",
    "    Input:\n",
    "        heatmaps: (batch_size, num_landmarks, height, width)\n",
    "    Output:\n",
    "        coordinates: (batch_size, num_landmarks, 2) - [x, y] coordinates\n",
    "    \"\"\"\n",
    "    b, n, h, w = heatmaps.shape\n",
    "    heatmaps = F.softmax(heatmaps.view(b, n, -1), dim=-1)  # Flatten and apply softmax\n",
    "    heatmaps = heatmaps.view(b, n, h, w)  # Reshape back to (b, n, h, w)\n",
    "    \n",
    "    # Create grid for coordinates\n",
    "    x = torch.linspace(0, w - 1, w).to(heatmaps.device)\n",
    "    y = torch.linspace(0, h - 1, h).to(heatmaps.device)\n",
    "    x_grid, y_grid = torch.meshgrid(x, y, indexing=\"xy\")\n",
    "\n",
    "    # Compute weighted sums for x and y\n",
    "    x_coords = torch.sum(heatmaps * x_grid[None, None, :, :], dim=(2, 3))\n",
    "    y_coords = torch.sum(heatmaps * y_grid[None, None, :, :], dim=(2, 3))\n",
    "\n",
    "    return torch.stack([x_coords, y_coords], dim=-1)  # (b, n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadedDViT(nn.Module):\n",
    "    def __init__(self, num_blocks, embed_dim, num_heads, num_landmarks):\n",
    "        super(CascadedDViT, self).__init__()\n",
    "        self.backbone = ModifiedResNet50()\n",
    "        \n",
    "        # Upsample ResNet output to match the spatial dimensions of DualViT input\n",
    "        self.upsample_resnet = nn.ConvTranspose2d(2048, embed_dim, kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "        # Create Prediction Blocks\n",
    "        self.prediction_blocks = nn.ModuleList(\n",
    "            [PredictionBlock(embed_dim, num_heads, num_landmarks) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # Merge concatenated outputs for subsequent blocks\n",
    "        self.merge_conv = nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Extract features using ResNet\n",
    "        resnet_output = self.backbone(x)  # ResNet output: (B, 2048, 8, 8)\n",
    "        resnet_output = self.upsample_resnet(resnet_output)  # Upsampled output: (B, embed_dim, 32, 32)\n",
    "    \n",
    "        predictions = []\n",
    "        prev_features = None\n",
    "\n",
    "        # Step 2: Process through PredictionBlocks\n",
    "        for i, block in enumerate(self.prediction_blocks):\n",
    "            if i == 0:\n",
    "                # First block gets only ResNet output\n",
    "                combined_features = resnet_output\n",
    "            else:\n",
    "                # Subsequent blocks get concatenated features\n",
    "                combined_features = torch.cat([resnet_output, prev_features], dim=1)  # (B, embed_dim * 2, 32, 32)\n",
    "                combined_features = self.merge_conv(combined_features)  # (B, embed_dim, 32, 32)\n",
    "\n",
    "            # Process the combined features through the PredictionBlock\n",
    "            prev_features, heatmaps = block(combined_features)\n",
    "            coords_pred = soft_argmax(heatmaps)\n",
    "            # Save the heatmaps\n",
    "            predictions.append((heatmaps, coords_pred))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWingLoss(nn.Module):\n",
    "    def __init__(self, omega=14, epsilon=1, theta=0.5, alpha=2.1):\n",
    "        \"\"\"\n",
    "        Adaptive Wing Loss\n",
    "        Args:\n",
    "            omega: Scaling factor (default: 14)\n",
    "            epsilon: Smoothing parameter (default: 1)\n",
    "            theta: Threshold for switching between linear and non-linear (default: 0.5)\n",
    "            alpha: Exponential decay factor for the nonlinear component (default: 2.1)\n",
    "        \"\"\"\n",
    "        super(AWingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute AWing loss between predicted heatmaps and ground truth.\n",
    "        Args:\n",
    "            pred: Predicted heatmaps (b, n, h, w)\n",
    "            target: Ground truth heatmaps (b, n, h, w)\n",
    "        Returns:\n",
    "            loss: Adaptive Wing Loss\n",
    "        \"\"\"\n",
    "        diff = target - pred  # Difference between ground truth and predictions\n",
    "        abs_diff = diff.abs()\n",
    "\n",
    "        # Non-linear component for abs_diff < theta\n",
    "        A = self.omega * (1 / (1 + (self.theta / self.epsilon) ** (self.alpha - target))) * (self.alpha - target) * ((self.theta / self.alpha) ** (self.alpha - target - 1)) * (1 / self.epsilon)\n",
    "        nonlinear_loss = self.omega * torch.log(1 + ((abs_diff / self.epsilon) ** (self.alpha - target)))\n",
    "\n",
    "        # Linear component for abs_diff >= theta\n",
    "        C = self.theta * A - self.omega * torch.log(1 + ((self.theta / self.epsilon) ** (self.alpha - target)))\n",
    "        linear_loss = A * abs_diff - C\n",
    "\n",
    "        # Combine losses\n",
    "        loss = torch.where(abs_diff < self.theta, nonlinear_loss, linear_loss)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth L1 loss for coordinate regression\n",
    "smooth_l1_loss = nn.SmoothL1Loss()\n",
    "\n",
    "# Training loss class\n",
    "class TrainingLoss(nn.Module):\n",
    "    def __init__(self, num_blocks, w, beta):\n",
    "        super(TrainingLoss, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.w = w\n",
    "        self.beta = beta\n",
    "        self.heatmap_loss = AWingLoss()\n",
    "\n",
    "    def forward(self, predictions, heatmaps_gt, coords_gt):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            predictions: List of predictions from all blocks [(coords, heatmaps), ...]\n",
    "            heatmaps_gt: Ground truth heatmaps (b, n, h, w)\n",
    "            coords_gt: Ground truth coordinates (b, n, 2)\n",
    "        Output:\n",
    "            total_loss: Combined loss\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        for j, (heatmaps_pred, coords_pred) in enumerate(predictions):\n",
    "            # Compute intermediate losses\n",
    "            coord_loss = smooth_l1_loss(coords_pred, coords_gt)\n",
    "            heatmap_loss = self.heatmap_loss(heatmaps_pred, heatmaps_gt)\n",
    "\n",
    "            # Combine losses with beta\n",
    "            intermediate_loss = coord_loss + self.beta * heatmap_loss\n",
    "\n",
    "            # Weight by w^(j-B)\n",
    "            weight = self.w ** (j - self.num_blocks)\n",
    "            total_loss += weight * intermediate_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmaps(keypoints, img_size, heatmap_size, sigma=2):\n",
    "    \"\"\"\n",
    "    Generate heatmaps for keypoints using the coordinate encoding method.\n",
    "    \n",
    "    Args:\n",
    "        keypoints (torch.Tensor): Keypoints of shape (N, num_keypoints, 2) - [(u, v)].\n",
    "        img_size (tuple): Original image size (height, width).\n",
    "        heatmap_size (tuple): Heatmap size (height, width).\n",
    "        sigma (float): Standard deviation for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Heatmaps of shape (N, num_keypoints, heatmap_height, heatmap_width).\n",
    "    \"\"\"\n",
    "    N, num_keypoints, _ = keypoints.shape\n",
    "    device = keypoints.device\n",
    "    heatmaps = torch.zeros((N, num_keypoints, heatmap_size[0], heatmap_size[1]), device=device, dtype=torch.float32)\n",
    "\n",
    "    # Downsampling ratio\n",
    "    lambda_x = img_size[1] / heatmap_size[1]  # width ratio\n",
    "    lambda_y = img_size[0] / heatmap_size[0]  # height ratio\n",
    "\n",
    "    for i in range(N):  # Iterate over batch\n",
    "        for j in range(num_keypoints):  # Iterate over each keypoint\n",
    "            u, v = keypoints[i, j]  # Original coordinates (u, v)\n",
    "\n",
    "            # Downsample coordinates\n",
    "            u_prime = u / lambda_x\n",
    "            v_prime = v / lambda_y\n",
    "            \n",
    "            # Quantize (can use floor, ceil, or round)\n",
    "            u_quant = torch.round(u_prime)\n",
    "            v_quant = torch.round(v_prime)\n",
    "\n",
    "            # Create Gaussian kernel centered at quantized location\n",
    "            y = torch.arange(0, heatmap_size[0], device=device, dtype=torch.float32)\n",
    "            x = torch.arange(0, heatmap_size[1], device=device, dtype=torch.float32)\n",
    "            y_grid, x_grid = torch.meshgrid(y, x, indexing=\"ij\")\n",
    "\n",
    "            heatmaps[i, j] = torch.exp(\n",
    "                -((x_grid - u_quant)**2 + (y_grid - v_quant)**2) / (2 * sigma**2)\n",
    "            )\n",
    "            heatmaps[i, j] /= (2 * np.pi * sigma**2)  # Normalize the Gaussian\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 407.9499\n",
      "Epoch 2/10, Loss: 405.7901\n",
      "Epoch 3/10, Loss: 405.7766\n",
      "Epoch 4/10, Loss: 405.7796\n",
      "Epoch 5/10, Loss: 405.7797\n",
      "Epoch 6/10, Loss: 405.7864\n",
      "Epoch 7/10, Loss: 405.7935\n",
      "Epoch 8/10, Loss: 405.7826\n",
      "Epoch 9/10, Loss: 405.7921\n",
      "Epoch 10/10, Loss: 405.7826\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:1\")\n",
    "# Initialize the model and move it to the correct device\n",
    "model = CascadedDViT(num_blocks=8, embed_dim=256, num_heads=8, num_landmarks=98).to(device)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = TrainingLoss(num_blocks=8, w=1.2, beta=0.5)\n",
    "\n",
    "# Original image size\n",
    "img_size = (256, 256)  # Assuming your input images are cropped and resized to 256x256.\n",
    "\n",
    "# Heatmap size\n",
    "heatmap_size = (32, 32)  # Matches the predicted heatmap size.\n",
    "\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, coords_gt in train_loader:  # Only images and keypoints are provided\n",
    "        # Ensure images and ground truth are moved to the correct device\n",
    "        images = images.to(device)  # Shape: (batch_size, 3, img_H, img_W)\n",
    "        coords_gt = coords_gt.to(device)  # Shape: (batch_size, num_landmarks, 2)\n",
    "        \n",
    "        # Generate ground truth heatmaps\n",
    "        heatmaps_gt = generate_heatmaps(coords_gt, img_size, heatmap_size)  # (batch_size, num_landmarks, heatmap_H, heatmap_W)\n",
    "        heatmaps_gt = heatmaps_gt.to(device)  # Move heatmaps to the correct device\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)  # Output: [(coords_pred, heatmaps_pred), ...]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, heatmaps_gt, coords_gt)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"dvit_keypoint_model.pth\")\n",
    "print(\"Training complete and model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
